# logistic-regression-from-scratch
ğŸ“„ Project Documentation: Logistic Regression via Perceptron Trick & Sklearn Comparison
Title: Logistic Regression: Manual Implementation with Perceptron Trick vs. Sklearn
Author's Objective:
To explore how Logistic Regression can be understood from scratch using the Perceptron learning rule, and compare the results with the sklearn implementation.

ğŸ§  Core Concepts and Flow
1. Logistic Regression Background
Purpose: Draw a linear boundary (decision line) between two classes.

Equation: 
ğ´ğ‘¥1+ğµğ‘¥2+ğ¶=0
Classes: Positive class (1) and Negative class (0)

ğŸ”§ Implementation Details
âœ… Data Generation:
100 2D points using make_classification from sklearn.
Linearly separable data with clear separation between classes.
âœ… Initial Line Assumption:
Start with a random line: 
ğ‘¥1+ğ‘¥2-1=0

Visualized using Matplotlib.

âœ… Perceptron Trick (Manual Implementation):
Introduces a step function to predict class.

Weights updated using:

new_weights =old_weights+ğ›¼(ğ‘¦_actualâˆ’ğ‘¦_pred)â‹…ğ‘‹

Simple rule for binary classification, similar to Perceptron logic.

Plotted the updated decision line based on final weights.

âœ… Drawback Noted:
Hard step function leads to non-differentiable updates.

Not ideal for logistic regression, which should use sigmoid activation and log loss.

ğŸ“Š Comparison with Sklearn Implementation
Trained logistic regression model from sklearn.linear_model.LogisticRegression

Compared coefficients and intercept with manual perceptron implementation.

Plotted both decision boundaries:

Black line: Perceptron trick

Red line: Sklearn logistic regression

âœ… Bonus: Sigmoid-Based Manual Update
Attempts a second manual implementation using sigmoid activation in the step function.

Compared all three boundaries:

Perceptron trick

Sklearn logistic regression

Perceptron + Sigmoid (Logistic regression inspired)
